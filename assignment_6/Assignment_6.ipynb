{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "\n",
    "Submitter : JESOON KANG, 20170937\n",
    "Date : 2019. 10. \n",
    "\n",
    "\n",
    "    Assignment 4. \n",
    "\n",
    "-   -\n",
    "\n",
    "\n",
    "'''\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torchvision\n",
    "import os\n",
    "\n",
    "\n",
    "EPSILON = 1.5 * 10**-12\n",
    "LEARNING_RATE = 0.0001\n",
    "BATCH_SIZE = 100\n",
    "FEATURE_SIZE = 10000\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#### Section 1. #### This Section is bringed Data_import_ex.py file.\n",
    "\n",
    "# Image Data import & resize\n",
    "\n",
    "transform = transforms.Compose([#transforms.Resize((256,256)),  \n",
    "                                transforms.Grayscale(),\t\t# the code transforms.Graysclae() is for changing the size [3,100,100] to [1, 100, 100] (notice : [channel, height, width] )\n",
    "                                transforms.ToTensor(),])\n",
    "\n",
    "#train_data_path = 'relative path of training data set'\n",
    "train_data_path = '../data/horse-or-human/train'\n",
    "trainset = torchvision.datasets.ImageFolder(root=train_data_path, transform=transform)\n",
    "# change the valuse of batch_size, num_workers for your program\n",
    "# if shuffle=True, the data reshuffled at every epoch \n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=100, shuffle=True, num_workers=0)  \n",
    "\n",
    "\n",
    "validation_data_path = '../data/horse-or-human/validation'\n",
    "valset = torchvision.datasets.ImageFolder(root=validation_data_path, transform=transform)\n",
    "# change the valuse of batch_size, num_workers for your program\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=100, shuffle=True, num_workers=0)  \n",
    "\n",
    "#### Section 1 END ####\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# sigmoid Function.\n",
    "def sigmoid(z) :\n",
    "    return 1 / (1 + torch.pow(math.e,-z)+ EPSILON)\n",
    "\n",
    "\n",
    "def get_activation(z,type) :\n",
    "    if (type == 0) :\n",
    "        return sigmoid(z)\n",
    "    else :\n",
    "        print(\"Error, get_activation\")\n",
    "        return 0\n",
    "\n",
    "def get_derv_activation(z,type) :\n",
    "    if (type == 0) :\n",
    "        return sigmoid(z)*(1-sigmoid(z))\n",
    "    else :\n",
    "        print(\"Error, get_derv_activation\")\n",
    "        return 0                                                                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Setting up Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# Below codes activates when want to use cpu\n",
    "device = torch.device('cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_origin_loss(y,a) :\n",
    "    ret = -(torch.div(y,a+EPSILON) - torch.div(1-y,1-a+EPSILON))\n",
    "    return ret\n",
    "\n",
    "def get_l2_loss(y,a,w_1,w_2,w_3) :\n",
    "    A = -(torch.div(y,a+EPSILON) - torch.div(1-y,1-a+EPSILON))\n",
    "    B = (LAMBDA/2)*(torch.sum(w_1**2)**(0.5) + torch.sum(w_2**2)**(0.5) + torch.sum(w_3**2)**(0.5))\n",
    "    ret = (1/a.shape[0])*A + B\n",
    "    return ret\n",
    "\n",
    "class ML :\n",
    "    def __init__(self,act1,act2,act3,L1_size,L2_size,L3_size,lr,min_loss_diff,LAMBDA=0.0001) :\n",
    "        self.act_type_1 = act1\n",
    "        self.act_type_2 = act2\n",
    "        self.act_type_3 = act3 \n",
    "        self.l1_size = L1_size\n",
    "        self.l2_size = L2_size\n",
    "        self.l3_size = L3_size\n",
    "        self.feature_size = 10000\n",
    "        self.epoch = 0\n",
    "        self.lr = lr\n",
    "        self.min_loss_diff = min_loss_diff\n",
    "        self.lamb = LAMBDA\n",
    "        self.init_weights()\n",
    "        \n",
    "        print(\"ML Object initialized\")\n",
    "        self.iter = 0\n",
    "\n",
    "        self.val_acc_log = []\n",
    "        self.val_loss_log = []\n",
    "        self.train_acc_log = []\n",
    "        self.train_loss_log = []\n",
    "        self.epoch_log = []\n",
    "        self.val_acc_log.append(0)\n",
    "        self.val_loss_log.append(0)\n",
    "        self.train_acc_log.append(0)\n",
    "        self.train_loss_log.append(0)\n",
    "        self.epoch_log.append(0)\n",
    "        \n",
    "    def init_weights(self) :\n",
    "        self.w_1 = torch.FloatTensor(self.feature_size,self.l1_size).uniform_(-1,1).to(device)\n",
    "        \n",
    "        self.b_1 = torch.FloatTensor(1,self.l1_size).uniform_(-1,1).to(device)\n",
    "        \n",
    "        #torch.FloatTensor(a, b).uniform_(r1, r2)\n",
    "        \n",
    "        self.w_2 = torch.FloatTensor(self.l1_size,self.l2_size).uniform_(-1,1).to(device)\n",
    "        self.b_2 = torch.FloatTensor(1,self.l2_size).uniform_(-1,1).to(device)\n",
    "\n",
    "        self.w_3 = torch.FloatTensor(self.l2_size,self.l3_size).uniform_(-1,1).to(device)\n",
    "        self.b_3 = torch.FloatTensor(1,self.l3_size).uniform_(-1,1).to(device)\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "    def training(self) :\n",
    "        \n",
    "        epoch = 0\n",
    "        for epoch in range(0,1000000):\n",
    "            train_acc_log_tmp = []\n",
    "            train_loss_log_tmp = []\n",
    "            val_acc_log_tmp = []\n",
    "            val_loss_log_tmp = []\n",
    "            \n",
    "\n",
    "        # load training images of the batch size for every iteration\n",
    "            for i, data in enumerate(trainloader):\n",
    "                \n",
    "                # inputs is the images\n",
    "                # labels is the class of the image\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.float() , labels.float()\n",
    "                # if you don't change the image size, it will be [batch_size, 1, 100, 100]\n",
    "\n",
    "                # if labels is horse it returns tensor[0,0,0] else it returns tensor[1,1,1]\n",
    "                \n",
    "                self.t_data_batch = torch.squeeze(torch.FloatTensor(data[0]),3)\n",
    "                \n",
    "                self.t_data_batch = self.t_data_batch.view(len(data[0]),FEATURE_SIZE).to(device)\n",
    "                #print(\"test : shape \", self.t_data_batch.shape)\n",
    "                \n",
    "                # this must added to NN-re file\n",
    "                self.batch_size = self.t_data_batch.shape[0]\n",
    "                \n",
    "                self.z_1 = torch.matmul(self.t_data_batch,self.w_1) + self.b_1\n",
    "                #print(self.z_1)\n",
    "                \n",
    "                \n",
    "                self.a_1 = get_activation(self.z_1,self.act_type_1)\n",
    "                #print(self.a_1)\n",
    "                \n",
    "                self.z_2 = torch.matmul(self.a_1,self.w_2) + self.b_2\n",
    "\n",
    "                self.a_2 = get_activation(self.z_2,self.act_type_2) # 1027 x 50\n",
    "\n",
    "                self.z_3 = torch.matmul(self.a_2,self.w_3) + self.b_3\n",
    "                #print(self.z_3)\n",
    "                self.a_3 = get_activation(self.z_3,self.act_type_3) # 1027 x 1\n",
    "            \n",
    "                #print(self.a_3)\n",
    "                self.t_yh_batch = data[1].float().unsqueeze(1).to(device)\n",
    "\n",
    "                #print(self.a_3,self.t_yh_batch)\n",
    "                acc = self.get_acc(self.a_3,self.t_yh_batch)\n",
    "                B = (self.lamb/2)*(torch.sum(self.w_1**2)**(0.5) + torch.sum(self.w_2**2)**(0.5) + torch.sum(self.w_3**2)**(0.5))\n",
    "                #print(B)\n",
    "                loss = get_origin_loss(self.t_yh_batch,self.a_3) + B\n",
    "                #print(loss)\n",
    "                loss = loss.mean()\n",
    "                train_acc_log_tmp.append(acc)\n",
    "                train_loss_log_tmp.append(loss)\n",
    "                \n",
    "                self.update_weights(self.t_yh_batch,self.a_3)\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "            train_acc = torch.tensor(train_acc_log_tmp).mean().item()\n",
    "            train_loss = torch.tensor(train_loss_log_tmp).mean().item()\n",
    "            print(\"Train| E: %s, Loss: %.9s, Acc:%.9s\"%(epoch,train_loss,train_acc))\n",
    "            \n",
    "           \n",
    "\n",
    "            # load validation images of the batch size for every iteration\n",
    "            for i, data in enumerate(valloader):\n",
    "\n",
    "                # inputs is the image\n",
    "                # labels is the class of the image\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.float() , labels.float()\n",
    "                # if you don't change the image size, it will be [batch_size, 1, 100, 100]\n",
    "                \n",
    "                \n",
    "                # if labels is horse it returns tensor[0,0,0] else it returns tensor[1,1,1]\n",
    "                \n",
    "                self.v_data_batch = torch.squeeze(torch.FloatTensor(data[0]),3)\n",
    "                \n",
    "                self.v_data_batch = self.v_data_batch.view(len(data[0]),FEATURE_SIZE).to(device)\n",
    "                self.batch_size = self.v_data_batch.shape[0]\n",
    "                self.z_1 = torch.matmul(self.v_data_batch,self.w_1) + self.b_1\n",
    "                \n",
    "                self.a_1 = get_activation(self.z_1,self.act_type_1)\n",
    "                #print(self.a_1)\n",
    "\n",
    "                self.z_2 = torch.matmul(self.a_1,self.w_2) + self.b_2\n",
    "                self.a_2 = get_activation(self.z_2,self.act_type_2) # 1027 x 50\n",
    "\n",
    "                self.z_3 = torch.matmul(self.a_2,self.w_3) + self.b_3\n",
    "                self.a_3 = get_activation(self.z_3,self.act_type_3) # 1027 x 1\n",
    "\n",
    "                self.v_yh_batch = data[1].float().unsqueeze(1).to(device)\n",
    "                acc = self.get_acc(self.a_3,self.v_yh_batch)\n",
    "                B = (self.lamb/2)*(torch.sum(self.w_1**2)**(0.5) + torch.sum(self.w_2**2)**(0.5) + torch.sum(self.w_3**2)**(0.5))\n",
    "                #print(B)\n",
    "                loss = get_origin_loss(self.v_yh_batch,self.a_3).mean() + B\n",
    "                \n",
    "                val_acc_log_tmp.append(acc)\n",
    "                val_loss_log_tmp.append(loss)\n",
    "            val_acc = torch.tensor(val_acc_log_tmp).mean().item()\n",
    "            val_loss = torch.tensor(val_loss_log_tmp).mean().item()\n",
    "            print(\"                                              VAL |E: %s, Loss : %.9s, ACC: %.9s\"%(epoch,val_loss,val_acc))\n",
    "\n",
    "            self.train_acc_log.append(train_acc)\n",
    "            self.train_loss_log.append(train_loss)\n",
    "            self.val_acc_log.append(val_acc)\n",
    "            self.val_loss_log.append(val_loss)\n",
    "            self.epoch_log.append(epoch)\n",
    "            epoch += 1\n",
    "            \n",
    "            tmp_idx = len(self.train_loss_log)-1\n",
    "            \n",
    "            loss_gap = abs(self.train_loss_log[tmp_idx]-self.train_loss_log[tmp_idx-1])\n",
    "            print(\"                                                                                          %10.8s : Loss gap\"%(loss_gap))\n",
    "            if ( loss_gap < self.min_loss_diff) :\n",
    "                print(\"Learning is terminated.\")\n",
    "                break     \n",
    "\n",
    "        \n",
    "    def update_weights(self,t_y,a_3) :\n",
    "        error_wb3 = -(torch.div(t_y,a_3+ EPSILON) - torch.div(1.0-t_y,1.0-a_3+ EPSILON))\n",
    "        #print(\"error_wb3.shape :\",error_wb3.shape)\n",
    "        d_z_3 = error_wb3*get_derv_activation(self.z_3,self.act_type_3) #\n",
    "        #print(\"d_z_3.shape : \",d_z_3.shape)\n",
    "        #print(\"a_2.shape :\",self.a_2.shape)\n",
    "        d_w_3 = torch.matmul(self.a_2.T,d_z_3)\n",
    "        d_b_3 = torch.sum(d_z_3, dim=0, keepdim=True) / self.a_2.shape[0] # mean도 됨\n",
    "        \n",
    "        #########\n",
    "        \n",
    "        error_wb2 = torch.matmul(d_z_3,self.w_3.T)\n",
    "        \n",
    "        d_z_2 = error_wb2*get_derv_activation(self.z_2,self.act_type_2)\n",
    "        #print(\"self.a_1 shape : \",self.a_1.shape)\n",
    "        #print(\"d_z_2.shape : \", d_z_2.shape)\n",
    "        d_w_2 = torch.matmul(self.a_1.T,d_z_2)\n",
    "        #print(\"d_w_2.shape : \",d_w_2.shape)\n",
    "        d_b_2 = torch.sum(d_z_2, dim=0,keepdims=True) / self.a_1.shape[0]\n",
    "        \n",
    "   \n",
    "        error_wb1 = torch.matmul(d_z_2,self.w_2.T)\n",
    "        d_z_1 = error_wb1*get_derv_activation(self.z_1,self.act_type_1)\n",
    "        d_w_1 = torch.matmul(self.t_data_batch.T, d_z_1) #100, 10000 / 100,10\n",
    "        d_b_1 = torch.sum(d_z_1, dim=0,keepdims=True) / self.t_data_batch.shape[0]\n",
    "        \n",
    "        \n",
    "        #print(\"d_w_3.shape :\", d_w_3.shape)\n",
    "        self.w_3 = (1 - (self.lr*self.lamb)/(self.batch_size))*self.w_3 - self.lr*d_w_3\n",
    "        self.b_3 += -self.lr*d_b_3\n",
    "        self.w_2 = (1 - (self.lr*self.lamb)/(self.batch_size))*self.w_2 - self.lr*d_w_2\n",
    "        self.b_2 += -self.lr*d_b_2\n",
    "        self.w_1 = (1 - (self.lr*self.lamb)/(self.batch_size))*self.w_1 -self.lr*d_w_1\n",
    "        self.b_1 += -self.lr*d_b_1\n",
    "        #print(b_3,b_2,b_1)\n",
    "        \n",
    "    def get_acc(self,yhat,y) :\n",
    "        count = 0\n",
    "      \n",
    "        for a,b in zip(yhat,y) :\n",
    "            if a >= 0.5 :\n",
    "                if b == 1 :\n",
    "                    count+=1\n",
    "            else :\n",
    "                if b == 0:\n",
    "                    count +=1\n",
    "        \n",
    "        return count / len(yhat)\n",
    "\n",
    "    def show_loss(self) :\n",
    "        #print(self.train_loss_log)\n",
    "        #print(self.val_loss_log)\n",
    "        #print(self.epoch_log)\n",
    "        \n",
    "        tmp_1 = torch.tensor(self.train_loss_log)\n",
    "        tmp_2 = torch.tensor(self.epoch_log)\n",
    "       \n",
    "        t1 = plt.plot(self.epoch_log,self.train_loss_log, color='orange',label='Training Loss')\n",
    "        t2 = plt.plot(self.epoch_log,self.val_loss_log, color= 'red',label='Validation Loss')\n",
    "        plt.title(\"Loss\")\n",
    "        plt.legend(['Training Loss','Validation Loss'])\n",
    "        plt.show()\n",
    "    def show_acc(self) :\n",
    "        t1 = plt.plot(self.epoch_log,self.train_acc_log, color='orange',label='Training Acc')\n",
    "        t2 = plt.plot(self.epoch_log,self.val_acc_log, color= 'red',label='Validation Acc')\n",
    "        plt.title(\"Accuracy\")\n",
    "        plt.legend(['Training Acc','Validation Acc'])\n",
    "        plt.show()\n",
    "    def get_final_result(self) :\n",
    "        print(\"Architecture info : \")\n",
    "        print(\"| Layer Size | Learning Rate | Lambda |LOSS_CONVERG_VAL|\")\n",
    "        print(\"|%3s %3s %3s |%15s|%8s|%16s|\"%(self.l1_size,self.l2_size,self.l3_size,self.lr,self.lamb,self.min_loss_diff))\n",
    "        print(\"_________________________________\")\n",
    "        print(\"|Dataset    | loss    | accuracy|\")\n",
    "        print(\"|training   |%9s|%9s|\"%(self.train_loss_log[-1],self.train_acc_log[-1]))\n",
    "        print(\"|validation |%9s|%9s|\"%(self.val_loss_log[-1],self.val_acc_log[-1]))\n",
    "        print(\"|_______________________________|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ML Object initialized\n",
      "ML Object initialized\n",
      "ML Object initialized\n"
     ]
    }
   ],
   "source": [
    "\n",
    "LEARNING_RATE = 0.00001\n",
    "MIN_LOSS_DIFF = 0.0001\n",
    "activation_type = 0 # 0 = sigmoid\n",
    "\n",
    "machine_1 = ML(0,0,0,50,10,1,LEARNING_RATE,MIN_LOSS_DIFF,LAMBDA=0.00001)\n",
    "machine_2 = ML(0,0,0,50,10,1,LEARNING_RATE,MIN_LOSS_DIFF,LAMBDA=0.001)\n",
    "machine_3 = ML(0,0,0,50,10,1,LEARNING_RATE,MIN_LOSS_DIFF,LAMBDA=0.01)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train| E: 0, Loss: -1.108631, Acc:0.4858586\n",
      "                                              VAL |E: 0, Loss : -0.715042, ACC: 0.4895237\n",
      "                                                                                            1.108631 : Loss gap\n",
      "Train| E: 1, Loss: -0.844761, Acc:0.4858586\n",
      "                                              VAL |E: 1, Loss : -0.497606, ACC: 0.5066666\n",
      "                                                                                            0.263870 : Loss gap\n",
      "Train| E: 2, Loss: -0.667671, Acc:0.4770370\n",
      "                                              VAL |E: 2, Loss : -0.362845, ACC: 0.5340476\n",
      "                                                                                            0.177089 : Loss gap\n",
      "Train| E: 3, Loss: -0.489159, Acc:0.4876767\n",
      "                                              VAL |E: 3, Loss : -0.236855, ACC: 0.5764285\n",
      "                                                                                            0.178512 : Loss gap\n",
      "Train| E: 4, Loss: -0.426252, Acc:0.5049495\n",
      "                                              VAL |E: 4, Loss : -0.134053, ACC: 0.6500000\n",
      "                                                                                            0.062906 : Loss gap\n",
      "Train| E: 5, Loss: -0.274958, Acc:0.5496969\n",
      "                                              VAL |E: 5, Loss : -0.048959, ACC: 0.6676190\n",
      "                                                                                            0.151293 : Loss gap\n",
      "Train| E: 6, Loss: -0.240713, Acc:0.5559596\n",
      "                                              VAL |E: 6, Loss : 0.0145712, ACC: 0.6907142\n",
      "                                                                                            0.034245 : Loss gap\n",
      "Train| E: 7, Loss: -0.176453, Acc:0.5636026\n",
      "                                              VAL |E: 7, Loss : -0.046953, ACC: 0.6835713\n",
      "                                                                                            0.064259 : Loss gap\n",
      "Train| E: 8, Loss: -0.180346, Acc:0.5635016\n",
      "                                              VAL |E: 8, Loss : 0.0190082, ACC: 0.7107143\n",
      "                                                                                            0.003892 : Loss gap\n",
      "Train| E: 9, Loss: -0.143633, Acc:0.5676767\n",
      "                                              VAL |E: 9, Loss : 0.0169320, ACC: 0.7240476\n",
      "                                                                                            0.036712 : Loss gap\n",
      "Train| E: 10, Loss: -0.130074, Acc:0.5807743\n",
      "                                              VAL |E: 10, Loss : 0.1117887, ACC: 0.7061905\n",
      "                                                                                            0.013558 : Loss gap\n",
      "Train| E: 11, Loss: -0.044986, Acc:0.5908753\n",
      "                                              VAL |E: 11, Loss : 0.1552519, ACC: 0.7133333\n",
      "                                                                                            0.085087 : Loss gap\n",
      "Train| E: 12, Loss: -0.036966, Acc:0.6026936\n",
      "                                              VAL |E: 12, Loss : 0.0644114, ACC: 0.7166666\n",
      "                                                                                            0.008020 : Loss gap\n",
      "Train| E: 13, Loss: -0.058047, Acc:0.5998653\n",
      "                                              VAL |E: 13, Loss : 0.0520603, ACC: 0.7114286\n",
      "                                                                                            0.021081 : Loss gap\n",
      "Train| E: 14, Loss: -0.013641, Acc:0.6036026\n",
      "                                              VAL |E: 14, Loss : 0.1340234, ACC: 0.6969048\n",
      "                                                                                            0.044405 : Loss gap\n",
      "Train| E: 15, Loss: -0.025569, Acc:0.5995960\n",
      "                                              VAL |E: 15, Loss : 0.1079929, ACC: 0.6783332\n",
      "                                                                                            0.011927 : Loss gap\n",
      "Train| E: 16, Loss: -0.024046, Acc:0.6016834\n",
      "                                              VAL |E: 16, Loss : 0.1336591, ACC: 0.6861904\n",
      "                                                                                            0.001523 : Loss gap\n",
      "Train| E: 17, Loss: -0.037004, Acc:0.6115151\n",
      "                                              VAL |E: 17, Loss : 0.1520268, ACC: 0.6809523\n",
      "                                                                                            0.012957 : Loss gap\n",
      "Train| E: 18, Loss: 0.0080238, Acc:0.6071380\n",
      "                                              VAL |E: 18, Loss : 0.1793977, ACC: 0.6940476\n",
      "                                                                                            0.045027 : Loss gap\n",
      "Train| E: 19, Loss: 0.0304581, Acc:0.5970370\n",
      "                                              VAL |E: 19, Loss : 0.1319729, ACC: 0.6895237\n",
      "                                                                                            0.022434 : Loss gap\n",
      "Train| E: 20, Loss: -0.021700, Acc:0.6160605\n",
      "                                              VAL |E: 20, Loss : 0.1280114, ACC: 0.6907142\n",
      "                                                                                            0.052158 : Loss gap\n",
      "Train| E: 21, Loss: -0.058049, Acc:0.6279798\n",
      "                                              VAL |E: 21, Loss : 0.1942058, ACC: 0.6697619\n",
      "                                                                                            0.036348 : Loss gap\n",
      "Train| E: 22, Loss: 0.0460115, Acc:0.6356229\n",
      "                                              VAL |E: 22, Loss : 0.1316009, ACC: 0.6888095\n",
      "                                                                                            0.104060 : Loss gap\n",
      "Train| E: 23, Loss: -0.015946, Acc:0.6144106\n",
      "                                              VAL |E: 23, Loss : 0.0825405, ACC: 0.6914286\n",
      "                                                                                            0.061958 : Loss gap\n",
      "Train| E: 24, Loss: -0.000270, Acc:0.6216834\n",
      "                                              VAL |E: 24, Loss : 0.1247129, ACC: 0.6954762\n",
      "                                                                                            0.015676 : Loss gap\n",
      "Train| E: 25, Loss: 0.0308242, Acc:0.6290572\n",
      "                                              VAL |E: 25, Loss : 0.1215594, ACC: 0.6935713\n",
      "                                                                                            0.031094 : Loss gap\n",
      "Train| E: 26, Loss: 0.0194750, Acc:0.6277778\n",
      "                                              VAL |E: 26, Loss : 0.0546544, ACC: 0.7021428\n",
      "                                                                                            0.011349 : Loss gap\n",
      "Train| E: 27, Loss: 0.0287836, Acc:0.6329629\n",
      "                                              VAL |E: 27, Loss : 0.1698168, ACC: 0.7133333\n",
      "                                                                                            0.009308 : Loss gap\n",
      "Train| E: 28, Loss: 0.0457510, Acc:0.6347811\n",
      "                                              VAL |E: 28, Loss : 0.0703380, ACC: 0.7114285\n",
      "                                                                                            0.016967 : Loss gap\n",
      "Train| E: 29, Loss: 0.0048467, Acc:0.6442424\n",
      "                                              VAL |E: 29, Loss : 0.0695640, ACC: 0.7128571\n",
      "                                                                                            0.040904 : Loss gap\n",
      "Train| E: 30, Loss: -0.035924, Acc:0.6399663\n",
      "                                              VAL |E: 30, Loss : 0.1553127, ACC: 0.7292857\n",
      "                                                                                            0.040771 : Loss gap\n",
      "Train| E: 31, Loss: 0.0265582, Acc:0.6485185\n",
      "                                              VAL |E: 31, Loss : 0.1512608, ACC: 0.7240476\n",
      "                                                                                            0.062483 : Loss gap\n",
      "Train| E: 32, Loss: 0.0401685, Acc:0.6487878\n",
      "                                              VAL |E: 32, Loss : 0.1156645, ACC: 0.7307143\n",
      "                                                                                            0.013610 : Loss gap\n",
      "Train| E: 33, Loss: -0.021998, Acc:0.6597979\n",
      "                                              VAL |E: 33, Loss : 0.1684390, ACC: 0.7307143\n",
      "                                                                                            0.062166 : Loss gap\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train| E: 34, Loss: -0.033553, Acc:0.6594276\n",
      "                                              VAL |E: 34, Loss : 0.1336448, ACC: 0.7326190\n",
      "                                                                                            0.011554 : Loss gap\n",
      "Train| E: 35, Loss: -0.084074, Acc:0.6603366\n",
      "                                              VAL |E: 35, Loss : 0.1175604, ACC: 0.7180953\n",
      "                                                                                            0.050520 : Loss gap\n",
      "Train| E: 36, Loss: 0.0111911, Acc:0.6587877\n",
      "                                              VAL |E: 36, Loss : 0.1120731, ACC: 0.7121428\n",
      "                                                                                            0.095265 : Loss gap\n",
      "Train| E: 37, Loss: 0.0109858, Acc:0.6569697\n",
      "                                              VAL |E: 37, Loss : 0.1099053, ACC: 0.7233333\n",
      "                                                                                            0.000205 : Loss gap\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method Image.__del__ of <PIL.Image.Image image mode=L size=100x100 at 0x7FB8900A36D8>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3/dist-packages/PIL/Image.py\", line 589, in __del__\n",
      "    if (hasattr(self, 'fp') and hasattr(self, '_exclusive_fp')\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train| E: 38, Loss: -0.003054, Acc:0.6648821\n",
      "                                              VAL |E: 38, Loss : 0.1160482, ACC: 0.7169048\n",
      "                                                                                            0.014040 : Loss gap\n",
      "Train| E: 39, Loss: -0.019204, Acc:0.6585185\n",
      "                                              VAL |E: 39, Loss : 0.1603221, ACC: 0.7307143\n",
      "                                                                                            0.016150 : Loss gap\n",
      "Train| E: 40, Loss: -0.011961, Acc:0.6446801\n",
      "                                              VAL |E: 40, Loss : 0.1399289, ACC: 0.7347618\n",
      "                                                                                            0.007243 : Loss gap\n",
      "Train| E: 41, Loss: 0.0207256, Acc:0.6569696\n",
      "                                              VAL |E: 41, Loss : 0.0394924, ACC: 0.7366666\n",
      "                                                                                            0.032687 : Loss gap\n",
      "Train| E: 42, Loss: -0.007133, Acc:0.6609764\n",
      "                                              VAL |E: 42, Loss : 0.1027037, ACC: 0.7426190\n",
      "                                                                                            0.027859 : Loss gap\n",
      "Train| E: 43, Loss: 0.0271230, Acc:0.6563298\n",
      "                                              VAL |E: 43, Loss : 0.1211219, ACC: 0.7414285\n",
      "                                                                                            0.034256 : Loss gap\n",
      "Train| E: 44, Loss: 0.0201789, Acc:0.6587878\n",
      "                                              VAL |E: 44, Loss : 0.0631889, ACC: 0.7485713\n",
      "                                                                                            0.006944 : Loss gap\n",
      "Train| E: 45, Loss: 0.0107696, Acc:0.6697979\n",
      "                                              VAL |E: 45, Loss : 0.0128465, ACC: 0.7459523\n",
      "                                                                                            0.009409 : Loss gap\n",
      "Train| E: 46, Loss: -0.015913, Acc:0.6617845\n",
      "                                              VAL |E: 46, Loss : 0.0784496, ACC: 0.7373809\n",
      "                                                                                            0.026682 : Loss gap\n",
      "Train| E: 47, Loss: -0.068569, Acc:0.6718855\n",
      "                                              VAL |E: 47, Loss : 0.0397396, ACC: 0.7504761\n",
      "                                                                                            0.052655 : Loss gap\n",
      "Train| E: 48, Loss: 0.0328620, Acc:0.6795286\n",
      "                                              VAL |E: 48, Loss : 0.0898138, ACC: 0.7340476\n",
      "                                                                                            0.101431 : Loss gap\n",
      "Train| E: 49, Loss: 0.0016332, Acc:0.6669697\n",
      "                                              VAL |E: 49, Loss : 0.0342456, ACC: 0.7366666\n",
      "                                                                                            0.031228 : Loss gap\n",
      "Train| E: 50, Loss: -0.032356, Acc:0.6681481\n",
      "                                              VAL |E: 50, Loss : 0.0546864, ACC: 0.7419047\n",
      "                                                                                            0.033989 : Loss gap\n",
      "Train| E: 51, Loss: -0.024273, Acc:0.6724241\n",
      "                                              VAL |E: 51, Loss : 0.0960361, ACC: 0.7538095\n",
      "                                                                                            0.008083 : Loss gap\n",
      "Train| E: 52, Loss: -0.010807, Acc:0.6585859\n",
      "                                              VAL |E: 52, Loss : 0.0613999, ACC: 0.7433333\n",
      "                                                                                            0.013465 : Loss gap\n",
      "Train| E: 53, Loss: -0.039503, Acc:0.6717845\n",
      "                                              VAL |E: 53, Loss : 0.0860901, ACC: 0.7302381\n",
      "                                                                                            0.028695 : Loss gap\n",
      "Train| E: 54, Loss: -0.033418, Acc:0.6650505\n",
      "                                              VAL |E: 54, Loss : 0.0845190, ACC: 0.7466666\n",
      "                                                                                            0.006085 : Loss gap\n",
      "Train| E: 55, Loss: -0.051953, Acc:0.6601347\n",
      "                                              VAL |E: 55, Loss : 0.0456590, ACC: 0.7414285\n",
      "                                                                                            0.018535 : Loss gap\n",
      "Train| E: 56, Loss: 0.0467752, Acc:0.6695958\n",
      "                                              VAL |E: 56, Loss : 0.0526587, ACC: 0.7538095\n",
      "                                                                                            0.098728 : Loss gap\n",
      "Train| E: 57, Loss: -0.007152, Acc:0.6834343\n",
      "                                              VAL |E: 57, Loss : 0.0299559, ACC: 0.7373809\n",
      "                                                                                            0.053927 : Loss gap\n",
      "Train| E: 58, Loss: -0.008531, Acc:0.6769697\n",
      "                                              VAL |E: 58, Loss : 0.0656270, ACC: 0.7426190\n",
      "                                                                                            0.001378 : Loss gap\n",
      "Train| E: 59, Loss: -0.020688, Acc:0.6837036\n",
      "                                              VAL |E: 59, Loss : 0.0585007, ACC: 0.7571428\n",
      "                                                                                            0.012156 : Loss gap\n",
      "Train| E: 60, Loss: -0.021609, Acc:0.6821548\n",
      "                                              VAL |E: 60, Loss : 0.1390442, ACC: 0.7440476\n",
      "                                                                                            0.000921 : Loss gap\n",
      "Train| E: 61, Loss: -0.030878, Acc:0.6919865\n",
      "                                              VAL |E: 61, Loss : 0.0784150, ACC: 0.7466666\n",
      "                                                                                            0.009268 : Loss gap\n",
      "Train| E: 62, Loss: -0.004484, Acc:0.6983501\n",
      "                                              VAL |E: 62, Loss : 0.0791957, ACC: 0.7611904\n",
      "                                                                                            0.026394 : Loss gap\n",
      "Train| E: 63, Loss: -0.050349, Acc:0.6882491\n",
      "                                              VAL |E: 63, Loss : 0.1103646, ACC: 0.7421428\n",
      "                                                                                            0.045864 : Loss gap\n",
      "Train| E: 64, Loss: -0.024028, Acc:0.6891582\n",
      "                                              VAL |E: 64, Loss : 0.0363236, ACC: 0.7526190\n",
      "                                                                                            0.026320 : Loss gap\n",
      "Train| E: 65, Loss: -0.004934, Acc:0.6826936\n",
      "                                              VAL |E: 65, Loss : 0.1399912, ACC: 0.7473809\n",
      "                                                                                            0.019093 : Loss gap\n",
      "Train| E: 66, Loss: 0.0038971, Acc:0.6863299\n",
      "                                              VAL |E: 66, Loss : 0.0809971, ACC: 0.7559523\n",
      "                                                                                            0.008832 : Loss gap\n",
      "Train| E: 67, Loss: -0.021210, Acc:0.6854208\n",
      "                                              VAL |E: 67, Loss : 0.1053401, ACC: 0.7323809\n",
      "                                                                                            0.025107 : Loss gap\n",
      "Train| E: 68, Loss: -0.003104, Acc:0.6890572\n",
      "                                              VAL |E: 68, Loss : 0.0862791, ACC: 0.7540476\n",
      "                                                                                            0.018106 : Loss gap\n"
     ]
    }
   ],
   "source": [
    "machine_1.training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "machine_2.training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "machine_3.training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "machine_1.get_final_result()\n",
    "machine_2.get_final_result()\n",
    "machine_3.get_final_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "machine_2.training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "machine_2.get_final_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "machine.show_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "machine.show_acc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "machine.get_final_result()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
