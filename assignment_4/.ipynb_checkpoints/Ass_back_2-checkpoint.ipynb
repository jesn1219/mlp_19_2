{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "\n",
    "Submitter : JESOON KANG, 20170937\n",
    "Date : 2019. 10. \n",
    "\n",
    "\n",
    "    Assignment 4. \n",
    "\n",
    "-   -\n",
    "\n",
    "\n",
    "'''\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torchvision\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict Function. if h(x) returns >=0.5, set to 1. other cases, set to 0.\n",
    "def predict(h, labels) :\n",
    "    mount = len(h)\n",
    "    correct = 0\n",
    "    \n",
    "    for i in range(0,mount) :\n",
    "        if h[i] >= 0.5 :\n",
    "            if labels[i] == 1 :\n",
    "                correct +=1\n",
    "        else :\n",
    "            if labels[i] == 0 :\n",
    "                correct +=1\n",
    "    return correct * (1/mount)\n",
    "\n",
    "#### Section 2 END ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#### Section 1. #### This Section is bringed Data_import_ex.py file.\n",
    "\n",
    "# Image Data import & resize\n",
    "\n",
    "transform = transforms.Compose([#transforms.Resize((256,256)),  \n",
    "                                transforms.Grayscale(),\t\t# the code transforms.Graysclae() is for changing the size [3,100,100] to [1, 100, 100] (notice : [channel, height, width] )\n",
    "                                transforms.ToTensor(),])\n",
    "\n",
    "#train_data_path = 'relative path of training data set'\n",
    "train_data_path = '../data/horse-or-human/train'\n",
    "trainset = torchvision.datasets.ImageFolder(root=train_data_path, transform=transform)\n",
    "# change the valuse of batch_size, num_workers for your program\n",
    "# if shuffle=True, the data reshuffled at every epoch \n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=100, shuffle=True, num_workers=0)  \n",
    "\n",
    "\n",
    "validation_data_path = '../data/horse-or-human/validation'\n",
    "valset = torchvision.datasets.ImageFolder(root=validation_data_path, transform=transform)\n",
    "# change the valuse of batch_size, num_workers for your program\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=100, shuffle=True, num_workers=0)  \n",
    "\n",
    "#### Section 1 END ####\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# sigmoid Function.\n",
    "def sigmoid(z) :\n",
    "    return 1 / (1 + torch.pow(math.e,-z)+0.000001)\n",
    "\n",
    "def tanh(z) :\n",
    "    ret = (math.e**z - math.e**-z)/(math.e**z + math.e**-z)\n",
    "    \n",
    "    return ret\n",
    "\n",
    "def derv_tanh(z) :\n",
    "    ret = 1 - (tanh(z)**2)\n",
    "    return ret\n",
    "\n",
    "def relu(z) :\n",
    "    ret = z.clone().detach()\n",
    "    ret[ret<=0] = 0\n",
    "    \n",
    "    return ret\n",
    "    \n",
    "    \n",
    "def derv_relu(z) :\n",
    "    ret = z.clone().detach()\n",
    "    ret[ret<=0] = 0\n",
    "    ret[ret>0] = 1\n",
    "    return ret\n",
    "    \n",
    "            \n",
    "def leakly_relu(z) :\n",
    "    ret = z.clone().detach()\n",
    "    ret= np.where(ret <=0, 0.01*ret,ret )\n",
    "    ret = torch.FloatTensor(ret)\n",
    "    \n",
    "    return ret\n",
    "\n",
    "def derv_leakly_relu(z) :\n",
    "    ret = z.clone().detach()\n",
    "    ret = np.where(ret<=0,0.01,1)\n",
    "    return ret\n",
    "\n",
    "def get_activation(z,type) :\n",
    "    if (type == 0) :\n",
    "        return sigmoid(z)\n",
    "    elif (type == 1) :\n",
    "        return tanh(z)\n",
    "    elif (type == 2) :\n",
    "        return relu(z)\n",
    "    elif (type == 3) :\n",
    "        return leakly_relu(z)\n",
    "    \n",
    "    else :\n",
    "        print(\"Error, get_activation\")\n",
    "        return 0\n",
    "\n",
    "def get_derv_activation(z,type) :\n",
    "    if (type == 0) :\n",
    "        return sigmoid(z)*(1-sigmoid(z))\n",
    "    elif (type == 1) :\n",
    "        return derv_tanh(z)\n",
    "    elif (type == 2) :\n",
    "        return derv_relu(z)\n",
    "    elif (type == 3) :\n",
    "        return derv_leakly_relu(z)    \n",
    "    else :\n",
    "        print(\"Error, get_derv_activation\")\n",
    "        return 0                                                                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(y,a) :\n",
    "    #print(y,a)\n",
    "    ret = -(torch.div(y,a+0.000001) - torch.div(1-y,1-a+0.000001))\n",
    "    \n",
    "    return ret\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "FEATURE_SIZE = 10000\n",
    "class ML :\n",
    "    def __init__(self,act1,act2,act3,L1_size,L2_size,L3_size,lr,min_loss_diff) :\n",
    "        self.act_type_1 = act1\n",
    "        self.act_type_2 = act2\n",
    "        self.act_type_3 = act3\n",
    "        \n",
    "        self.l1_size = L1_size\n",
    "        self.l2_size = L2_size\n",
    "        self.l3_size = L3_size\n",
    "        self.feature_size = 10000\n",
    "        self.batch_size = 256\n",
    "        self.t_batch_idx = 0\n",
    "        self.epoch = 0\n",
    "        self.lr = lr\n",
    "        self.min_loss_diff = min_loss_diff\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "        print(\"ML Object initialized\")\n",
    "        self.iter = 0\n",
    "\n",
    "        self.val_acc_log = []\n",
    "        self.val_loss_log = []\n",
    "        self.train_acc_log = []\n",
    "        self.train_loss_log = []\n",
    "        self.epoch_log = []\n",
    "        self.val_acc_log.append(0)\n",
    "        self.val_loss_log.append(0)\n",
    "        self.train_acc_log.append(0)\n",
    "        self.train_loss_log.append(0)\n",
    "        self.epoch_log.append(0)\n",
    "        \n",
    "    def init_weights(self) :\n",
    "        self.w_1 = torch.FloatTensor(self.l1_size,self.feature_size).uniform_(-1,1)\n",
    "        self.b_1 = torch.FloatTensor(1,self.l1_size).uniform_(-1,1)\n",
    "        \n",
    "        #torch.FloatTensor(a, b).uniform_(r1, r2)\n",
    "        \n",
    "        self.w_2 = torch.FloatTensor(self.l2_size,self.l1_size).uniform_(-1,1)\n",
    "        self.b_2 = torch.FloatTensor(1,self.l2_size).uniform_(-1,1)\n",
    "\n",
    "        self.w_3 = torch.FloatTensor(self.l3_size,self.l2_size).uniform_(-1,1)\n",
    "        self.b_3 = torch.FloatTensor(1,self.l3_size).uniform_(-1,1)\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "    def training(self) :\n",
    "        \n",
    "        epoch = 0\n",
    "        for epoch in range(0,1000000):\n",
    "            train_acc_log_tmp = []\n",
    "            train_loss_log_tmp = []\n",
    "            val_acc_log_tmp = []\n",
    "            val_loss_log_tmp = []\n",
    "            \n",
    "\n",
    "        # load training images of the batch size for every iteration\n",
    "            for i, data in enumerate(trainloader):\n",
    "                \n",
    "                # inputs is the images\n",
    "                # labels is the class of the image\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.float() , labels.float()\n",
    "                # if you don't change the image size, it will be [batch_size, 1, 100, 100]\n",
    "\n",
    "                # if labels is horse it returns tensor[0,0,0] else it returns tensor[1,1,1]\n",
    "                \n",
    "                self.t_data_batch = torch.squeeze(torch.FloatTensor(data[0]),3)\n",
    "                \n",
    "                self.t_data_batch = self.t_data_batch.view(len(data[0]),FEATURE_SIZE)\n",
    "                self.z_1 = torch.matmul(self.t_data_batch,self.w_1.T) + self.b_1\n",
    "\n",
    "                self.a_1 = get_activation(self.z_1,self.act_type_1)\n",
    "\n",
    "\n",
    "                self.z_2 = torch.matmul(self.a_1,self.w_2.T) + self.b_2\n",
    "                self.a_2 = get_activation(self.z_2,self.act_type_2) # 1027 x 50\n",
    "\n",
    "                self.z_3 = torch.matmul(self.a_2,self.w_3.T) + self.b_3\n",
    "                self.a_3 = get_activation(self.z_3,self.act_type_3) # 1027 x 1\n",
    "\n",
    "                self.t_yh_batch = data[1].float().unsqueeze(1)\n",
    "\n",
    "                #print(self.a_3,self.t_yh_batch)\n",
    "                acc = self.get_acc(self.a_3,self.t_yh_batch)\n",
    "                loss = np.array(get_loss(self.t_yh_batch, self.a_3)).mean()\n",
    "                \n",
    "                train_acc_log_tmp.append(acc)\n",
    "                train_loss_log_tmp.append(loss)\n",
    "                \n",
    "                self.update_weights(self.t_yh_batch,self.a_3)\n",
    "            \n",
    "            \n",
    "            \n",
    "            train_acc = np.array(train_acc_log_tmp).mean()\n",
    "            train_loss = np.array(train_loss_log_tmp).mean()\n",
    "\n",
    "            #print(\"epoch : %s, loss : %s, tra_acc : %s\"%(epoch,train_loss,train_acc))\n",
    "            \n",
    "           \n",
    "\n",
    "            # load validation images of the batch size for every iteration\n",
    "            for i, data in enumerate(valloader):\n",
    "\n",
    "                # inputs is the image\n",
    "                # labels is the class of the image\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.float() , labels.float()\n",
    "                # if you don't change the image size, it will be [batch_size, 1, 100, 100]\n",
    "                \n",
    "                \n",
    "                # if labels is horse it returns tensor[0,0,0] else it returns tensor[1,1,1]\n",
    "                \n",
    "                self.v_data_batch = torch.squeeze(torch.FloatTensor(data[0]),3)\n",
    "                \n",
    "                self.v_data_batch = self.v_data_batch.view(len(data[0]),FEATURE_SIZE)\n",
    "                \n",
    "                self.z_1 = torch.matmul(self.v_data_batch,self.w_1.T) + self.b_1\n",
    "\n",
    "                self.a_1 = get_activation(self.z_1,self.act_type_1)\n",
    "                print(self.a_1)\n",
    "\n",
    "                self.z_2 = torch.matmul(self.a_1,self.w_2.T) + self.b_2\n",
    "                self.a_2 = get_activation(self.z_2,self.act_type_2) # 1027 x 50\n",
    "\n",
    "                self.z_3 = torch.matmul(self.a_2,self.w_3.T) + self.b_3\n",
    "                self.a_3 = get_activation(self.z_3,self.act_type_3) # 1027 x 1\n",
    "\n",
    "                self.v_yh_batch = data[1].float().unsqueeze(1)\n",
    "                acc = self.get_acc(self.a_3,self.v_yh_batch)\n",
    "                \n",
    "                \n",
    "                loss = np.array(get_loss(self.v_yh_batch, self.a_3)).mean()\n",
    "                val_acc_log_tmp.append([acc])\n",
    "                val_loss_log_tmp.append([loss])\n",
    "            val_acc = np.array(val_acc_log_tmp).mean()\n",
    "            val_loss = np.array(val_loss_log_tmp).mean()\n",
    "            \n",
    "            #print(\"epoch : %s, loss : %s, val_acc : %s\"%(epoch,val_loss,val_acc))\n",
    "\n",
    "            self.train_acc_log.append(train_acc)\n",
    "            self.train_loss_log.append(train_loss)\n",
    "            self.val_acc_log.append(val_acc)\n",
    "            self.val_loss_log.append(val_loss)\n",
    "            self.epoch_log.append(epoch)\n",
    "            epoch += 1\n",
    "            \n",
    "            tmp_idx = len(self.train_loss_log)-1\n",
    "            if ( abs(self.train_loss_log[tmp_idx]-self.train_loss_log[tmp_idx-1]) < self.min_loss_diff) :\n",
    "                print(\"Learning is terminated.\")\n",
    "                break\n",
    "\n",
    "        \n",
    "    def update_weights(self,t_y,a_3) :\n",
    "        error_wb3 = -(torch.div(t_y,a_3) - torch.div(1.0-t_y,1.0-a_3)) # sum ep\n",
    "        d_z_3 = error_wb3*get_derv_activation(self.z_3,self.act_type_3) #         \n",
    "        d_w_3 = torch.matmul(d_z_3.T,self.a_2)\n",
    "        d_b_3 = torch.sum(d_z_3, dim=0, keepdim=True) / self.a_2.shape[0] # mean도 됨\n",
    "        \n",
    "        #########\n",
    "        \n",
    "        error_wb2 = torch.matmul(d_z_3,self.w_3)\n",
    "        \n",
    "        d_z_2 = error_wb2*get_derv_activation(self.z_2,self.act_type_2)\n",
    "        \n",
    "        d_w_2 = torch.matmul(d_z_2.T,self.a_1)\n",
    "        d_b_2 = torch.sum(d_z_2, dim=0,keepdims=True) / self.a_1.shape[0]\n",
    "        \n",
    "   \n",
    "        error_wb1 = torch.matmul(d_z_2,self.w_2)\n",
    "        d_z_1 = error_wb1*get_derv_activation(self.z_1,self.act_type_1)\n",
    "        d_w_1 = torch.matmul(d_z_1.T,self.t_data_batch)\n",
    "        d_b_1 = torch.sum(d_z_1, dim=0,keepdims=True) / self.t_data_batch.shape[0]\n",
    "        \n",
    "        self.w_3 += -self.lr*d_w_3\n",
    "        self.b_3 += -self.lr*d_b_3\n",
    "        \n",
    "        self.w_2 += -self.lr*d_w_2\n",
    "        self.b_2 += -self.lr*d_b_2\n",
    "        self.w_1 += -self.lr*d_w_1\n",
    "        self.b_1 += -self.lr*d_b_1\n",
    "        #print(b_3,b_2,b_1)\n",
    "        \n",
    "    def get_acc(self,yhat,y) :\n",
    "        count = 0\n",
    "      \n",
    "        for a,b in zip(yhat,y) :\n",
    "            if a >= 0.5 :\n",
    "                if b == 1 :\n",
    "                    count+=1\n",
    "            else :\n",
    "                if b == 0:\n",
    "                    count +=1\n",
    "        \n",
    "        return count / len(yhat)\n",
    "\n",
    "    def show_loss(self) :\n",
    "        #print(self.train_loss_log)\n",
    "        #print(self.val_loss_log)\n",
    "        #print(self.epoch_log)\n",
    "        \n",
    "        tmp_1 = torch.tensor(self.train_loss_log)\n",
    "        tmp_2 = torch.tensor(self.epoch_log)\n",
    "       \n",
    "        t1 = plt.plot(tmp_2,tmp_1, color='orange',label='Training Loss')\n",
    "        t2 = plt.plot(self.epoch_log,self.val_loss_log, color= 'red',label='Validation Loss')\n",
    "        plt.title(\"Loss\")\n",
    "        plt.legend(['Training Loss','Validation Loss'])\n",
    "        plt.show()\n",
    "    def show_acc(self) :\n",
    "        t1 = plt.plot(self.epoch_log,self.train_acc_log, color='orange',label='Training Acc')\n",
    "        t2 = plt.plot(self.epoch_log,self.val_acc_log, color= 'red',label='Validation Acc')\n",
    "        plt.title(\"Accuracy\")\n",
    "        plt.legend(['Training Acc','Validation Acc'])\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ML Object initialized\n"
     ]
    }
   ],
   "source": [
    "\n",
    "activation_type = 0 # 0 = sigmoid\n",
    "learningRate = 0.001\n",
    "min_loss_diff = 0.00001\n",
    "machine = ML(1,1,0,60,30,1,learningRate,min_loss_diff)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-5b50f3175314>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmachine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-4e21bb840bbb>\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;31m# load training images of the batch size for every iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \u001b[0;31m# inputs is the images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/var/pyenv/versions/3.7.3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/var/pyenv/versions/3.7.3/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/var/pyenv/versions/3.7.3/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/var/pyenv/versions/3.7.3/lib/python3.7/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    136\u001b[0m         \"\"\"\n\u001b[1;32m    137\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/var/pyenv/versions/3.7.3/lib/python3.7/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mdefault_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maccimage_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpil_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/var/pyenv/versions/3.7.3/lib/python3.7/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mpil_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/var/pyenv/versions/3.7.3/lib/python3.7/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    932\u001b[0m         \"\"\"\n\u001b[1;32m    933\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 934\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    935\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"P\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/var/pyenv/versions/3.7.3/lib/python3.7/site-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadonly\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exclusive_fp\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_close_exclusive_fp_after_loading\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/var/pyenv/versions/3.7.3/lib/python3.7/site-packages/PIL/PngImagePlugin.py\u001b[0m in \u001b[0;36mload_end\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 682\u001b[0;31m                 \u001b[0mcid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpng\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    683\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSyntaxError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/var/pyenv/versions/3.7.3/lib/python3.7/site-packages/PIL/PngImagePlugin.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0mcid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m             \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m             \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "machine.training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "machine.show_loss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "machine.show_acc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
